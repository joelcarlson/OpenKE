{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Notebook to Create Embeddings\n",
    "\n",
    "In this notebook we will create embeddings for the `FB15k` data.\n",
    "\n",
    "We will then use these embeddings to initialize a new model for **out-of-vocabulary** data. This new model will freeze all of the entity and relation embeddings that are present in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import models\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder which contains, minimally, 3 files:\n",
    "# 1. entity2id.txt\n",
    "# 2. relation2id.txt\n",
    "# 3. train2id.txt\n",
    "data_path = \"./benchmarks/FB15K/\"\n",
    "\n",
    "# Where we will save the model file and embeddings, respectively\n",
    "train_file_path = \"./res/model.vec.tf\"\n",
    "train_embedding_path = \"./res/embedding.vec.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run ComplEx To Create initial embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Imported train\n",
      "WARNING:root:Got train total: 483142\n",
      "WARNING:root:Got test total: 0\n",
      "WARNING:root:Got val total: 0\n",
      "WARNING:root:Set batch size: 2415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/envs/PBG/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/PBG/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/joelcarl/WorkDocs/Projects/SmartWaterfall/03_Embeddings/OpenKE/models/ComplEx.py:47: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/joelcarl/WorkDocs/Projects/SmartWaterfall/03_Embeddings/OpenKE/models/ComplEx.py:47: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/PBG/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/PBG/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/PBG/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/PBG/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Method:\n",
    "\n",
    "Create ComplEx embeddings for data in `data_path`\n",
    "\"\"\"\n",
    "\n",
    "# See http://proceedings.mlr.press/v48/trouillon16.pdf for some \n",
    "# justification for Adam, and ent and rel neg rates, and alpha\n",
    "con = config.Config()\n",
    "con.set_in_path(data_path)\n",
    "con.set_work_threads(8) # cores\n",
    "con.set_train_times(5) # Number of Epochs\n",
    "con.set_nbatches(200) # batches/epoch. We may wish to alter the code to instead allow setting of n_obs per batch, which is easier to interpret\n",
    "con.set_dimension(100) # dimension of embedding (real+im)\n",
    "con.set_ent_neg_rate(10) # \n",
    "con.set_rel_neg_rate(5) #\n",
    "con.set_lmbda(0) # l2 Regularization penalty\n",
    "\n",
    "con.set_alpha(0.001) \n",
    "con.set_opt_method(\"Adam\")\n",
    "\n",
    "#Models will be exported via tf.Saver() automatically.\n",
    "con.set_export_files(train_file_path, 10) # How many train steps between saving json file\n",
    "#Model parameters will be exported to json files automatically.\n",
    "con.set_out_files(train_embedding_path)\n",
    "\n",
    "con.init()\n",
    "#Set the knowledge embedding model\n",
    "con.set_model(models.ComplEx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 138.63, time: 42.0, mag: 0.011, sd: 0.012, [-0.072, 0.057]\n",
      "Epoch: 1, loss: 108.16, time: 41.0, mag: 0.098, sd: 0.113, [-0.442, 0.44]\n",
      "Epoch: 2, loss: 41.91, time: 41.0, mag: 0.239, sd: 0.259, [-0.795, 0.801]\n",
      "Epoch: 3, loss: 25.81, time: 40.0, mag: 0.293, sd: 0.319, [-1.095, 1.079]\n",
      "Epoch: 4, loss: 19.19, time: 39.0, mag: 0.325, sd: 0.355, [-1.23, 1.2]\n"
     ]
    }
   ],
   "source": [
    "#Train the model.\n",
    "con.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run ComplEx_Freeze using Embeddings produced in above step\n",
    "\n",
    "I have produced a minimal graph dataset of new **out-of-vocabulary** entities, and relations. This data can be found in the folder `FB15K_OOV`\n",
    "\n",
    "Let's create new embeddings for this data by freezing the embeddings produced above, and training the new graph into the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_path = \"./benchmarks/FB15K_OOV/\"\n",
    "\n",
    "# Where we will save the model file and embeddings, respectively\n",
    "new_file_path = \"./res/new/model.vec.tf\"\n",
    "new_embedding_path = \"./res/new/embedding.vec.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Imported train\n",
      "WARNING:root:Got train total: 19\n",
      "WARNING:root:Got test total: 0\n",
      "WARNING:root:Got val total: 0\n",
      "WARNING:root:Set batch size: 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New entities found:\n",
      "-- Total Entities in embedding file: 14951\n",
      "-- Total Entities in data: 14970 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Initialized embeddings from: ./res/embedding.vec.json\n",
      "WARNING:root:real_idx 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New relationships found:\n",
      "-- Total Relationships in embedding file: 1345\n",
      "-- Total Relationships in data: 1354 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:h dim: (304,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/joelcarl/WorkDocs/Projects/SmartWaterfall/03_Embeddings/OpenKE/models/ComplEx_freeze.py:99: calling reduce_logsumexp_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/joelcarl/WorkDocs/Projects/SmartWaterfall/03_Embeddings/OpenKE/models/ComplEx_freeze.py:99: calling reduce_logsumexp_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/PBG/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/PBG/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:root:Res dim: (304,)\n"
     ]
    }
   ],
   "source": [
    "con = config.Config()\n",
    "con.set_in_path(new_data_path)\n",
    "con.set_work_threads(8) # cores\n",
    "con.set_train_times(10) # 10 Seems to be around the time convergence mostly happens.\n",
    "con.set_nbatches(1) # batches/epoch. We may wish to alter the code to set n_obs per batch, which is easier to interpret\n",
    "con.set_dimension(100) # dimension of embedding (real+im)\n",
    "con.set_ent_neg_rate(10) # \n",
    "con.set_rel_neg_rate(5) #\n",
    "con.set_lmbda(0) # l2 Regularization penalty\n",
    "\n",
    "con.set_alpha(0.001) \n",
    "con.set_opt_method(\"Adam\")\n",
    "\n",
    "\n",
    "# Here we initialize the embeddings with the embeddings produced above\n",
    "# The embeddings must be a .json file with keys \"ent_embeddings\", and \"rel_embeddings\"\n",
    "con.set_freeze_train_embeddings(True)\n",
    "con.set_embedding_initializer_path(train_embedding_path)\n",
    "\n",
    "con.set_export_files(new_file_path, 10) # How many train steps between saving json file\n",
    "con.set_out_files(new_embedding_path)\n",
    "\n",
    "con.init()\n",
    "#Set the knowledge embedding model\n",
    "con.set_model(models.ComplEx_freeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 61.34, time: 0.0, mag: 0.32, sd: 0.401, [-1.176, 1.302]\n",
      "Epoch: 1, loss: 75.78, time: 0.0, mag: 0.322, sd: 0.396, [-1.175, 1.303]\n",
      "Epoch: 2, loss: 61.1, time: 0.0, mag: 0.324, sd: 0.403, [-1.175, 1.304]\n",
      "Epoch: 3, loss: 67.29, time: 0.0, mag: 0.32, sd: 0.399, [-1.174, 1.305]\n",
      "Epoch: 4, loss: 59.33, time: 0.0, mag: 0.324, sd: 0.406, [-1.174, 1.306]\n",
      "Epoch: 5, loss: 57.98, time: 0.0, mag: 0.32, sd: 0.403, [-1.174, 1.307]\n",
      "Epoch: 6, loss: 52.37, time: 0.0, mag: 0.32, sd: 0.401, [-1.173, 1.308]\n",
      "Epoch: 7, loss: 45.19, time: 0.0, mag: 0.327, sd: 0.407, [-1.173, 1.309]\n",
      "Epoch: 8, loss: 58.51, time: 0.0, mag: 0.317, sd: 0.396, [-1.173, 1.31]\n",
      "Epoch: 9, loss: 55.78, time: 0.0, mag: 0.32, sd: 0.404, [-1.172, 1.311]\n"
     ]
    }
   ],
   "source": [
    "#Train the model.\n",
    "con.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New embeddings for the out-of-vocabulary entities and relations are now created, and a file is saved that contains both new and old embeddings at the specified path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare new and old embeddings\n",
    "\n",
    "Here we will examine the embeddings used to initialize the new training, to the embeddings produced by the initialized model.\n",
    "\n",
    "Our goal is to confirm that embeddings in the training data were successfully frozen, and do not change after retraining. \n",
    "\n",
    "We expect:\n",
    "\n",
    "- Any entity or relationship that was created in the training data should be the same\n",
    "- Any entity or relationship that was created in the training data, but involved in the test data should still be the same\n",
    "- Any entity or relationship that was not created in training should be different\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(train_embedding_path, \"r\") as f: \n",
    "    old_embeddings = json.loads(f.read())\n",
    "    old_ent_embeddings = old_embeddings[\"ent_embeddings\"]\n",
    "    old_rel_embeddings = old_embeddings[\"rel_embeddings\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(new_embedding_path, \"r\") as f: \n",
    "    new_embeddings = json.loads(f.read())\n",
    "    new_ent_embeddings = new_embeddings[\"ent_embeddings\"]\n",
    "    new_rel_embeddings = new_embeddings[\"rel_embeddings\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Entities that should be the same are all entities up to and including entity 14950\n",
    "# (the maximum entity id in train2id)\n",
    "# e.g. 1018, 1234, 4169\n",
    "print(old_ent_embeddings[1233] == new_ent_embeddings[1233])\n",
    "print(old_ent_embeddings[1234] == new_ent_embeddings[1234])\n",
    "print(old_ent_embeddings[1235] == new_ent_embeddings[1235])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No embedding for entity 14951 in train embeddings\n",
      "Embedding for entity 14951 in new embeddings:\n",
      "[0.2992086708545685, -0.10801243782043457, -0.5317354798316956, -0.6309638619422913, 0.3361939787864685, 0.7201786041259766, -0.011285864748060703, -0.8057375550270081, 0.22400900721549988, -0.2980792224407196]\n"
     ]
    }
   ],
   "source": [
    "# Entities that are new should have embeddings in the new data, but not in the old\n",
    "\n",
    "# Check 14950 to confirm\n",
    "entity_id = 14951\n",
    "\n",
    "try:\n",
    "    print(old_ent_embeddings[entity_id])\n",
    "except IndexError:\n",
    "    print(\"No embedding for entity {} in train embeddings\".format(entity_id))\n",
    "    \n",
    "print(\"Embedding for entity {} in new embeddings:\".format(entity_id))    \n",
    "print(new_ent_embeddings[entity_id][0:10])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Relationships that should be the same are all entities up to and including 1344\n",
    "# (the maximum relation id in train2id)\n",
    "# e.g. 38, 58, 135\n",
    "print(old_rel_embeddings[57] == new_rel_embeddings[57])\n",
    "print(old_rel_embeddings[58] == new_rel_embeddings[58])\n",
    "print(old_rel_embeddings[59] == new_rel_embeddings[59])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No relationship embedding for 1345 in train embeddings\n",
      "Embedding for relationship 1345 in new embeddings:\n",
      "[0.0993245393037796, -0.6665051579475403, -0.0976848378777504, 0.19483378529548645, -0.6981079578399658, -0.2871093153953552, 0.2919817864894867, -0.31209680438041687, -0.07304192334413528, 0.31981706619262695]\n"
     ]
    }
   ],
   "source": [
    "# Relationships that are new should have embeddings in the new data, but not in the old\n",
    "\n",
    "# Check 1344 to confirm\n",
    "rel_id = 1345\n",
    "\n",
    "try:\n",
    "    print(old_rel_embeddings[rel_id])\n",
    "except IndexError:\n",
    "    print(\"No relationship embedding for {} in train embeddings\".format(rel_id))\n",
    "    \n",
    "print(\"Embedding for relationship {} in new embeddings:\".format(rel_id))    \n",
    "print(new_rel_embeddings[rel_id][0:10])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lengths should be as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14951\n",
      "14970\n",
      "1345\n",
      "1354\n"
     ]
    }
   ],
   "source": [
    "print(len(old_ent_embeddings))\n",
    "print(len(new_ent_embeddings))\n",
    "print(len(old_rel_embeddings))\n",
    "print(len(new_rel_embeddings))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PBG]",
   "language": "python",
   "name": "conda-env-PBG-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
