{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Appropriate Dictionary Files from ECAD Data\n",
    "\n",
    "In this document I will convert Joes ecad files into appropriate *2id.txt files\n",
    "\n",
    "These are:\n",
    "\n",
    "1. entity2id\n",
    "2. relation2id\n",
    "3. train2id.txt\n",
    "\n",
    "Each of these is required by the OpenKE library to function properly\n",
    "\n",
    "Furthermore, we must convert the embeddings from the initial training phase into an embedding file consumable by OpenKE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "ecad_train_graph_path = \"/Users/joelcarl/Downloads/pybiggraph-20190516-230034.tsv\"\n",
    "ecad_train_embeddings_path = \"/Users/joelcarl/Downloads/ecad_gensim_embeddings.txt\" # Where the embeddings are now\n",
    "\n",
    "train2id_path = \"/Users/joelcarl/Desktop/train2id.txt\"\n",
    "entity2id_path = \"/Users/joelcarl/Desktop/entity2id.txt\"\n",
    "relation2id_path = \"/Users/joelcarl/Desktop/relation2id.txt\"\n",
    "ecad_openKE_embeddings_path = \"/Users/joelcarl/Desktop/ecad_gensim_embeddings.json\" # Where the OpenKE compatible embeddings should go\n",
    "\n",
    "# TEST\n",
    "ecad_test_graph_path = \"/Users/joelcarl/Downloads/pybiggraph-20190517-080024.tsv\"\n",
    "test_data_path = \"/Users/joelcarl/Desktop/test/\"\n",
    "\n",
    "train2id_test_path = test_data_path + \"train2id.txt\"\n",
    "entity2id_test_path = test_data_path + \"entity2id.txt\"\n",
    "relation2id_test_path = test_data_path + \"relation2id.txt\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Convert train graph items into ids\n",
    "\n",
    "The files are organized as text files, where each line is a tab separated `entity\\trelationship\\tentity`\n",
    "\n",
    "What we will do is go over the file to create two dictionaries: \n",
    "    1. An index of entity to monotonically increasing id\n",
    "    2. An index of relationship to monotonically increasing id\n",
    "\n",
    "at the same time we will write a new file, train2id.txt on the fly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write training graph with indices instead of names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity2id_dict = {}\n",
    "relation2id_dict = {}\n",
    "i = 1\n",
    "ent_id = 0\n",
    "rel_id = 0\n",
    "num_train_graph_lines = sum(1 for line in open(ecad_train_graph_path))\n",
    "\n",
    "with open(ecad_train_graph_path) as f:  \n",
    "    with open(train2id_path, \"w\") as train2id_f:        \n",
    "        # By convention, the count of the num lines must be at the top of the file\n",
    "        train2id_f.write(str(num_train_graph_lines) + \"\\n\")        \n",
    "        for line in f:\n",
    "            # Get entities, add to dicts based on increasing id\n",
    "            ent_1, rel, ent_2 = line.strip(\"\\n\").split(\"\\t\")\n",
    "            if ent_1 not in entity2id_dict:\n",
    "                entity2id_dict[ent_1] = ent_id\n",
    "                ent_id += 1\n",
    "            if ent_2 not in entity2id_dict:\n",
    "                entity2id_dict[ent_2] = ent_id\n",
    "                ent_id += 1\n",
    "            if rel not in relation2id_dict:\n",
    "                relation2id_dict[rel] = rel_id\n",
    "                rel_id += 1            \n",
    "            \n",
    "            # Write to file:\n",
    "            train2id_f.write(\"\\t\".join([str(item) for item in [entity2id_dict[ent_1], entity2id_dict[ent_2], relation2id_dict[rel]]]) + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write relation2id and entity2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write relation2id\n",
    "with open(relation2id_path, \"w\") as relation2id_f: \n",
    "    sorted_rel = sorted(relation2id_dict.items(), key=lambda kv: kv[1])\n",
    "    relation2id_f.write(str(len(sorted_rel)) + \"\\n\")  \n",
    "    for rel, idx in sorted_rel:\n",
    "        relation2id_f.write(\"\\t\".join([rel, str(idx)]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write entity2id\n",
    "with open(entity2id_path, \"w\") as entity2id_f: \n",
    "    sorted_ent = sorted(entity2id_dict.items(), key=lambda kv: kv[1])\n",
    "    entity2id_f.write(str(len(sorted_ent)) + \"\\n\")  \n",
    "    for ent, idx in sorted_ent:\n",
    "        entity2id_f.write(\"\\t\".join([ent, str(idx)]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Embeddings into OpenKE Format\n",
    "\n",
    "We must convert the embeddings file into the format:\n",
    "\n",
    "`{\"ent_embeddings\": [[..., ..., ..., ], ], \"rel_embeddings\": [[..., ..., ..., ], ]}`\n",
    "\n",
    "In the order of the rel and entity dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding_dict = {}\n",
    "with open(ecad_train_embeddings_path, \"r\") as f:\n",
    "    next(f) # Skip header\n",
    "    for line in f:\n",
    "        l_splt = line.strip(\"\\n\").split(\" \")\n",
    "        # First value is either an entity or relation, remaining values are embedding values\n",
    "        train_embedding_dict[l_splt[0]] = [float(num) for num in l_splt[1:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of embeddings in OpenKE format:\n",
    "embedding_dict = {\"ent_embeddings\":[train_embedding_dict[ent] for ent, idx in sorted_ent],\n",
    "                  \"rel_embeddings\":[train_embedding_dict[rel] for rel, idx in sorted_rel]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump to json\n",
    "with open(ecad_openKE_embeddings_path, 'w') as fp:\n",
    "    json.dump(embedding_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Convert Test Graph items into Appropriate Files\n",
    "\n",
    "For the test graph, we only want to write instances that are **NOT** in the train graph, same goes for entity and relationships. However, we must be careful to maintain the appropriate indices. That is, the new entity and relationship ids must continue from the training indices. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: You can restart the kernel here, just make sure to run the top cell to get paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing dictionaries\n",
    "entity2id_dict = {}\n",
    "with open(entity2id_path, \"r\") as entity2id_f: \n",
    "    next(entity2id_f) # Skip header\n",
    "    for line in entity2id_f:\n",
    "        ent, idx = line.strip(\"\\n\").split(\"\\t\")\n",
    "        entity2id_dict[ent] = int(idx)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation2id_dict = {}\n",
    "with open(relation2id_path, \"r\") as relation2id_f: \n",
    "    next(relation2id_f) # Skip header\n",
    "    for line in relation2id_f:\n",
    "        rel, idx = line.strip(\"\\n\").split(\"\\t\")\n",
    "        relation2id_dict[rel] = int(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get starting indices\n",
    "\n",
    "# with open(train2id_path, \"r\") as train2id_f:  \n",
    "#     train2id_idx = int(train2id_f.readline().strip(\"\\n\"))\n",
    "\n",
    "with open(relation2id_path, \"r\") as relation2id_f: \n",
    "    rel_id = int(relation2id_f.readline().strip(\"\\n\"))\n",
    "    \n",
    "with open(entity2id_path, \"r\") as entity2id_f:     \n",
    "    ent_id = int(entity2id_f.readline().strip(\"\\n\"))\n",
    "    \n",
    "# print(\"train2id start index:\\t\\t\"+str(train2id_idx))\n",
    "print(\"relation2id start index:\\t\"+str(rel_id))\n",
    "print(\"entity2id start index:\\t\\t\"+str(ent_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_graph_lines = sum(1 for line in open(ecad_test_graph_path))\n",
    "\n",
    "with open(ecad_test_graph_path) as f:  \n",
    "    with open(train2id_test_path, \"w\") as train2id_f:        \n",
    "        # By convention, the count of the num lines must be at the top of the file\n",
    "        train2id_f.write(str(num_test_graph_lines) + \"\\n\")        \n",
    "        for line in f:\n",
    "            write_to_file = False\n",
    "            # Get entities, add to dicts based on increasing id\n",
    "            ent_1, rel, ent_2 = line.strip(\"\\n\").split(\"\\t\")\n",
    "            if ent_1 not in entity2id_dict:\n",
    "                entity2id_dict[ent_1] = ent_id\n",
    "                ent_id += 1\n",
    "                write_to_file = True\n",
    "            if ent_2 not in entity2id_dict:\n",
    "                entity2id_dict[ent_2] = ent_id\n",
    "                ent_id += 1\n",
    "                write_to_file = True\n",
    "            if rel not in relation2id_dict:\n",
    "                relation2id_dict[rel] = rel_id\n",
    "                rel_id += 1            \n",
    "                write_to_file = True\n",
    "                \n",
    "            # Write to file **only** if one of the entities or rel are not in the training file:\n",
    "            if write_to_file:\n",
    "                train2id_f.write(\"\\t\".join([str(item) for item in [entity2id_dict[ent_1], entity2id_dict[ent_2], relation2id_dict[rel]]]) + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write relation2id\n",
    "with open(relation2id_test_path, \"w\") as relation2id_f: \n",
    "    sorted_rel = sorted(relation2id_dict.items(), key=lambda kv: kv[1])\n",
    "    relation2id_f.write(str(len(sorted_rel)) + \"\\n\")  \n",
    "    for rel, idx in sorted_rel:\n",
    "        relation2id_f.write(\"\\t\".join([rel, str(idx)]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write entity2id\n",
    "with open(entity2id_test_path, \"w\") as entity2id_f: \n",
    "    sorted_ent = sorted(entity2id_dict.items(), key=lambda kv: kv[1])\n",
    "    entity2id_f.write(str(len(sorted_ent)) + \"\\n\")  \n",
    "    for ent, idx in sorted_ent:\n",
    "        entity2id_f.write(\"\\t\".join([ent, str(idx)]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path = \"/Users/joelcarl/Desktop/test/\"\n",
    "\n",
    "\n",
    "test_file_path = \"/Users/joelcarl/Desktop/test/model_new.vec.tf\"\n",
    "test_embedding_path = \"/Users/joelcarl/Desktop/test/embedding_new.vec.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import models\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='7'\n",
    "#Input training files from benchmarks/FB15K/ folder.\n",
    "con = config.Config()\n",
    "#True: Input test files from the same folder.\n",
    "con.set_in_path(test_data_path)\n",
    "con.set_test_link_prediction(False)\n",
    "con.set_test_triple_classification(False)\n",
    "con.set_work_threads(8)\n",
    "con.set_train_times(10)\n",
    "con.set_nbatches(6) # Total obs = 18 . Need ~3 per batch, so batch size = total / n_batches \n",
    "con.set_alpha(0.001)\n",
    "con.set_margin(1.0)\n",
    "con.set_bern(0)\n",
    "con.set_dimension(400) # dimension of embedding\n",
    "con.set_ent_neg_rate(1)\n",
    "con.set_rel_neg_rate(0)\n",
    "con.set_opt_method(\"SGD\")\n",
    "\n",
    "\n",
    "con.set_freeze_train_embeddings(True)\n",
    "con.set_embedding_initializer_path(ecad_openKE_embeddings_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Imported train\n",
      "WARNING:root:Got train total: 418\n",
      "WARNING:root:Got test total: 0\n",
      "WARNING:root:Got val total: 0\n",
      "WARNING:root:Set batch size: 69\n",
      "WARNING:root:Initialized embeddings from: /Users/joelcarl/Desktop/ecad_gensim_embeddings.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New entities found:\n",
      "-- Total Entities in embedding file: 20\n",
      "-- Total Entities in data: 5000 \n",
      "WARNING:tensorflow:From /anaconda3/envs/PBG/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/PBG/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:root:Initialized embddings in transE\n",
      "WARNING:root:made it to fake pos r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/joelcarl/WorkDocs/Scratch/PBG/OpenKE_round2/OpenKE/models/TransE_freeze.py:119: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/joelcarl/WorkDocs/Scratch/PBG/OpenKE_round2/OpenKE/models/TransE_freeze.py:119: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/PBG/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/PBG/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/PBG/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/PBG/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "#Models will be exported via tf.Saver() automatically.\n",
    "con.set_export_files(test_file_path, 0)\n",
    "#Model parameters will be exported to json files automatically.\n",
    "con.set_out_files(test_embedding_path)\n",
    "#Initialize experimental settings.\n",
    "con.init()\n",
    "#Set the knowledge embedding model\n",
    "con.set_model(models.TransE_freeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 811.2923736572266, time: 0.0\n",
      "Epoch: 1, loss: 691.4817504882812, time: 0.0\n",
      "Epoch: 2, loss: 691.9060363769531, time: 0.0\n",
      "Epoch: 3, loss: 769.8413848876953, time: 0.0\n",
      "Epoch: 4, loss: 721.5551605224609, time: 7.152557373046875e-07\n",
      "Epoch: 5, loss: 781.9990234375, time: 9.5367431640625e-07\n",
      "Epoch: 6, loss: 744.8305206298828, time: 0.0\n",
      "Epoch: 7, loss: 764.2288513183594, time: 0.0\n",
      "Epoch: 8, loss: 606.1986999511719, time: 0.0\n",
      "Epoch: 9, loss: 435.43772888183594, time: 0.0\n"
     ]
    }
   ],
   "source": [
    "#Train the model.\n",
    "con.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PBG] *",
   "language": "python",
   "name": "conda-env-PBG-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
