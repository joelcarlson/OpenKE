{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Appropriate Dictionary Files from ECAD Data\n",
    "\n",
    "In this document I will convert Joes ecad files into appropriate *2id.txt files\n",
    "\n",
    "These are:\n",
    "\n",
    "1. entity2id\n",
    "2. relation2id\n",
    "3. train2id.txt\n",
    "\n",
    "Each of these is required by the OpenKE library to function properly\n",
    "\n",
    "Furthermore, we must convert the embeddings from the initial training phase into an embedding file consumable by OpenKE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "ecad_train_graph_path = \"/Users/joelcarl/Downloads/pybiggraph-20190516-230034.tsv\"\n",
    "ecad_train_embeddings_path = \"/Users/joelcarl/Downloads/ecad_gensim_embeddings.txt\" # Where the embeddings are now\n",
    "\n",
    "\n",
    "\n",
    "train2id_path = \"/Users/joelcarl/Desktop/train2id.txt\"\n",
    "entity2id_path = \"/Users/joelcarl/Desktop/entity2id.txt\"\n",
    "relation2id_path = \"/Users/joelcarl/Desktop/relation2id.txt\"\n",
    "ecad_openKE_embeddings_path = \"/Users/joelcarl/Desktop/ecad_gensim_embeddings.json\" # Where the OpenKE compatible embeddings should go\n",
    "\n",
    "# TEST\n",
    "ecad_test_graph_path = \"/Users/joelcarl/Downloads/pybiggraph-20190517-080024.tsv\"\n",
    "\n",
    "train2id_test_path = \"/Users/joelcarl/Desktop/test/train2id.txt\"\n",
    "entity2id_test_path = \"/Users/joelcarl/Desktop/test/entity2id.txt\"\n",
    "relation2id_test_path = \"/Users/joelcarl/Desktop/test/relation2id.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Convert train graph items into ids\n",
    "\n",
    "The files are organized as text files, where each line is a tab separated `entity\\trelationship\\tentity`\n",
    "\n",
    "What we will do is go over the file to create two dictionaries: \n",
    "    1. An index of entity to monotonically increasing id\n",
    "    2. An index of relationship to monotonically increasing id\n",
    "\n",
    "at the same time we will write a new file, train2id.txt on the fly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write training graph with indices instead of names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity2id_dict = {}\n",
    "relation2id_dict = {}\n",
    "i = 1\n",
    "ent_id = 0\n",
    "rel_id = 0\n",
    "num_train_graph_lines = sum(1 for line in open(ecad_train_graph_path))\n",
    "\n",
    "with open(ecad_train_graph_path) as f:  \n",
    "    with open(train2id_path, \"w\") as train2id_f:        \n",
    "        # By convention, the count of the num lines must be at the top of the file\n",
    "        train2id_f.write(str(num_train_graph_lines) + \"\\n\")        \n",
    "        for line in f:\n",
    "            # Get entities, add to dicts based on increasing id\n",
    "            ent_1, rel, ent_2 = line.strip(\"\\n\").split(\"\\t\")\n",
    "            if ent_1 not in entity2id_dict:\n",
    "                entity2id_dict[ent_1] = ent_id\n",
    "                ent_id += 1\n",
    "            if ent_2 not in entity2id_dict:\n",
    "                entity2id_dict[ent_2] = ent_id\n",
    "                ent_id += 1\n",
    "            if rel not in relation2id_dict:\n",
    "                relation2id_dict[rel] = rel_id\n",
    "                rel_id += 1            \n",
    "            \n",
    "            # Write to file:\n",
    "            train2id_f.write(\"\\t\".join([str(item) for item in [entity2id_dict[ent_1], entity2id_dict[ent_2], relation2id_dict[rel]]]) + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write relation2id and entity2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write relation2id\n",
    "with open(relation2id_path, \"w\") as relation2id_f: \n",
    "    sorted_rel = sorted(relation2id_dict.items(), key=lambda kv: kv[1])\n",
    "    relation2id_f.write(str(len(sorted_rel)) + \"\\n\")  \n",
    "    for rel, idx in sorted_rel:\n",
    "        relation2id_f.write(\"\\t\".join([rel, str(idx)]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write entity2id\n",
    "with open(entity2id_path, \"w\") as entity2id_f: \n",
    "    sorted_ent = sorted(entity2id_dict.items(), key=lambda kv: kv[1])\n",
    "    entity2id_f.write(str(len(sorted_ent)) + \"\\n\")  \n",
    "    for ent, idx in sorted_ent:\n",
    "        entity2id_f.write(\"\\t\".join([ent, str(idx)]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Embeddings into OpenKE Format\n",
    "\n",
    "We must convert the embeddings file into the format:\n",
    "\n",
    "`{\"ent_embeddings\": [[..., ..., ..., ], ], \"rel_embeddings\": [[]]}`\n",
    "\n",
    "In the order of the rel and entity dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    embs = open(embedding_path, 'r')\n",
    "# Store configuration file values\n",
    "except FileNotFoundError:\n",
    "    raise Exception('Entity embedding file not found: {}'.format(embedding_path))\n",
    "\n",
    "embedding_dict = json.loads(embs.read())\t\n",
    "ent_embedding = embedding_dict[\"ent_embeddings\"]\n",
    "self.ent_embedding_length = len(ent_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding_dict = {}\n",
    "with open(ecad_train_embeddings_path, \"r\") as f:\n",
    "    next(f) # Skip header\n",
    "    for line in f:\n",
    "        l_splt = line.strip(\"\\n\").split(\"\\s\")\n",
    "        train_embedding_dict[l_splt[0]] = l_splt[1:] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"ent_embeddings\": [[0.042046695947647095, 0.00392%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Convert Test Graph items into Appropriate Files\n",
    "\n",
    "For the test graph, we only want to write instances that are **NOT** in the train graph, same goes for entity and relationships. However, we must be careful to maintain the appropriate indices. That is, the new entity and relationship ids must continue from the training indices. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: You can restart the kernel here, just make sure to run the top cell to get paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing dictionaries\n",
    "entity2id_dict = {}\n",
    "with open(entity2id_path, \"r\") as entity2id_f: \n",
    "    next(entity2id_f) # Skip header\n",
    "    for line in entity2id_f:\n",
    "        ent, idx = line.strip(\"\\n\").split(\"\\t\")\n",
    "        entity2id_dict[ent] = int(idx)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation2id_dict = {}\n",
    "with open(relation2id_path, \"r\") as relation2id_f: \n",
    "    next(relation2id_f) # Skip header\n",
    "    for line in relation2id_f:\n",
    "        rel, idx = line.strip(\"\\n\").split(\"\\t\")\n",
    "        relation2id_dict[rel] = int(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation2id start index:\t3\n",
      "entity2id start index:\t\t4580\n"
     ]
    }
   ],
   "source": [
    "# Get starting indices\n",
    "\n",
    "# with open(train2id_path, \"r\") as train2id_f:  \n",
    "#     train2id_idx = int(train2id_f.readline().strip(\"\\n\"))\n",
    "\n",
    "with open(relation2id_path, \"r\") as relation2id_f: \n",
    "    rel_id = int(relation2id_f.readline().strip(\"\\n\"))\n",
    "    \n",
    "with open(entity2id_path, \"r\") as entity2id_f:     \n",
    "    ent_id = int(entity2id_f.readline().strip(\"\\n\"))\n",
    "    \n",
    "# print(\"train2id start index:\\t\\t\"+str(train2id_idx))\n",
    "print(\"relation2id start index:\\t\"+str(rel_id))\n",
    "print(\"entity2id start index:\\t\\t\"+str(ent_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_graph_lines = sum(1 for line in open(ecad_test_graph_path))\n",
    "\n",
    "with open(ecad_test_graph_path) as f:  \n",
    "    with open(train2id_test_path, \"w\") as train2id_f:        \n",
    "        # By convention, the count of the num lines must be at the top of the file\n",
    "        train2id_f.write(str(num_test_graph_lines) + \"\\n\")        \n",
    "        for line in f:\n",
    "            write_to_file = False\n",
    "            # Get entities, add to dicts based on increasing id\n",
    "            ent_1, rel, ent_2 = line.strip(\"\\n\").split(\"\\t\")\n",
    "            if ent_1 not in entity2id_dict:\n",
    "                entity2id_dict[ent_1] = ent_id\n",
    "                ent_id += 1\n",
    "                write_to_file = True\n",
    "            if ent_2 not in entity2id_dict:\n",
    "                entity2id_dict[ent_2] = ent_id\n",
    "                ent_id += 1\n",
    "                write_to_file = True\n",
    "            if rel not in relation2id_dict:\n",
    "                relation2id_dict[rel] = rel_id\n",
    "                rel_id += 1            \n",
    "                write_to_file = True\n",
    "                \n",
    "            # Write to file **only** if one of the entities or rel are not in the training file:\n",
    "            if write_to_file:\n",
    "                train2id_f.write(\"\\t\".join([str(item) for item in [entity2id_dict[ent_1], entity2id_dict[ent_2], relation2id_dict[rel]]]) + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write relation2id\n",
    "with open(relation2id_test_path, \"w\") as relation2id_f: \n",
    "    sorted_rel = sorted(relation2id_dict.items(), key=lambda kv: kv[1])\n",
    "    relation2id_f.write(str(len(sorted_rel)) + \"\\n\")  \n",
    "    for rel, idx in sorted_rel:\n",
    "        relation2id_f.write(\"\\t\".join([rel, str(idx)]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write entity2id\n",
    "with open(entity2id_test_path, \"w\") as entity2id_f: \n",
    "    sorted_ent = sorted(entity2id_dict.items(), key=lambda kv: kv[1])\n",
    "    entity2id_f.write(str(len(sorted_ent)) + \"\\n\")  \n",
    "    for ent, idx in sorted_ent:\n",
    "        entity2id_f.write(\"\\t\".join([ent, str(idx)]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PBG] *",
   "language": "python",
   "name": "conda-env-PBG-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
